\section{Related Work}
\label{sec-related}
We survey work related to our study of factors affecting bugginess
(namely, time-of-week); prior work on commit empirical studies; studies
of bugginess in distributed software development; and empirical studies 
on bug lifetime.

\paragraph{Day of Week of Commits}
The most closely related work, due to \'Sliwerski et al, studied the day of week of commits for two totally different projects, Eclipse and Mozilla, and
found that the commits on Fridays are buggiest~\cite{sliwerski-msr-2005}. 
This paper differs from the work of \'Sliwerski et al in the following key three aspects.
Firstly, we investigated how the commits' time of day correlates with the bugginess of commits, 
which has not been studied before to the best of our knowledge.
Secondly, we studied the correlation between commit bugginess and developers' activity levels as well as 
developer experience, which the previous work did not study.
Lastly, we used different data collection techniques. Specifically, we did not reply on the link 
between a commit and a bug report to extract bug-fixing commits, which enabled us to study 
software for which such links are not maintained or not well maintained by the developers. For example, we 
found that only 2.3\% of the bug-fixing commits are linked to a bug report, 
through manually examining a random sample of our bug-fixing commits in the Linux kernel.
While using links between bug reports and bug commits may increase the precision of extracting 
bug-fixing commits, our results demonstrate that 
high precision can be obtained without using such links: 
the precision of our bug-fixing commit extraction techniques are 
\linuxP for the Linux kernel and \postP for PostgreSQL.
%(1/43= 2.3\%)

\paragraph{Empirical Studies on Commits}
Many empirical studies were conducted to understand and leverage different aspects of 
commits~\cite{hattori2008nature,largeCommits,commitTextualClassification, smallCommits05, Swanson76}, which 
studied the distributions of commit sizes, how the commit sizes correlate with other metrics 
such as different development activities, commit classifications, etc. 

Hindle et. al~\cite{largeCommits} classified commits into different categories, one of which 
is non-functional commits (e.g., modification of comments, documentations, etc.).
Our study is different from it because we focused on
comment-fixing commits. For example, a refactoring commit 
%that modifies the source code but not comments 
would be considered as a non-functional commit by previous study, but not considered as a comment-fixing commits in this paper, 
thus being excluded from our study. 
iComment~\cite{iComment} only showed that FreeBSD contains many comment-fixing commits, and it is 
not a comprehensive study on comment-fixing commits.  
Our results show that 1,223 and 131 commits (Section~\ref{sec-comment-only}) %estimated from a random sample) 
in the Linux kernel and PostgreSQL only fix comments, and do not modify the code, showing
that developers spent time maintaining the correctness of comments. 


\paragraph{Distributed Software Development and Code Quality}
Several studies were performed to understand how the distributed software development affect 
code quality~\cite{distributed09, organizational08, global06} in open source and commercial 
software. While the two pieces of software we studied are open-source and developed in a distributed fashion,  
the goal of this paper is different from these prior studies as we aim to understand the correlation
between code bugginess and social characteristics of commits, e.g., commits' time of day, commits' day 
of week, developers' experience, and developers' activity frequencies, etc. 

\paragraph{Bug Lifetime}
Engler et. al~\cite{deviant} examined the bug lifetime of the Linux kernel in 2001. 
Our study on the bug lifetime complements theirs by analyzing the recent commits to the Linux kernel from 2005-2010.
Kim and Whitehead~\cite{2006-long} examined the bug lifetime of PostgreSQL.
Neither of the two studied the social characteristics such as commit time and author
experience. 


%\paragraph{Predicing code defects}

\comment{%-------- commit classification, cited above-------
\paragraph{On the Nature of Commits}
This paper studies the nature of commits in two dimensions: define the
size of commits in terms of number of files, and classify commits
based on the content of their comments \cite{hattori2008nature}. The
authors investigated the distribution of commits according to the
number of files, and their results show that the majority of commits
contain a large number of files. The authors also developed a
classification system for commits according to development and
maintenance activities based on the content of their commits, a system
that is more suitable for open source projects. Some of the major
findings made by the authors include: the majority of the commits are
not related to the development activities, corrective actions generate
more tiny commits, and development activities are spread among all
sizes of commits.
}

\comment{%------ to patrick: cited it in methods (about git blame) -------
\paragraph{Automatic identification of bug-introducing changes}
The authors try to automate the process of finding bug-introducing
changes, which would remove the manual work associated with going
through bug reports or commit logs to collect this type of
information. They use the SZZ algorithm, which traces from the
location of the fix where the bug was introduced, and as such extract
the time \cite{2006-automatic}.The weakness with this algorithm is
that sometimes it makes false detections because not all modifications
are fixes, and a moderate improvement from using just SZZ is the use
of annotation graphs. Another improvement was ignoring format changes,
which reduce a large number of false positives. This paper has
inspired the approach we used for automating the process of
identifying bug-introducing changes, which was key for collecting a
large pool of data.
}


\comment{ % -------- not that related ------
\paragraph{How long did it take to fix bugs?}
In this paper the author try to measure software quality as a function
of the number of bugs. The authors examine the bug fix time of files
in two open source software: ArgoUML and PostgreSQL, and tackle this
issue by identifying when bugs are introduced and when the bugs are
fixed \cite{2006-long}. The argument is files with the greatest
bug-fix times, whose bug counts are greater than average, may need
more attention to determine why bug fixes take such a long time –
potentially indicating the need for code refactoring to achieve faster
bug fixes in the future. The authors first extracted change histories
of the two projects, ArgoUML and PostgreSQL, using the Kenyon
infrastructure. To identify a bug-fix, they searched for keywords such
as fixed or bugs and they also searched for references to bug
reports. They then applied the identified bug-introducing changes by
applying the fix-inducing change identification algorithms described
in the paper “When do changes induce fixes?” which I mentioned
earlier.  This work is of course relevant because it gives us a
methodology for extracting bug-fix times.

%-------- replaced by their later work zimmermann-promise-2007, cited in intro
\paragraph{If your bug database could talk}
This is another relevant work, where the authors perform experiments
that demonstrate how to relate developer, code, and process to defects
in the code. This work tries to understand why some programs are more
failure-prone than others.  To answer this question, we have to know
which programs are more failure-prone than others – to search for
properties of the programs or its development process that commonly
correlate with defect density. The authors try to answer questions
like “can one predict failure-proneness from metrics like code
complexity?”, “what does a high number of bugs found after release?”,
and “do some developers write more failure-prone code than
others?”. After examining Eclipse database, some of the conclusions
that the authors made were: new or combination of existing metrics
need to be explored to study the relationship between complexity of
code to the presence of bugs in a given class, it is difficult to
predict post-release failures solely from process measurements, and
there is a high variance in failure density in files owned by
different developers \cite{2006-if}. Their methodology for extracting
information from bug reports from BUGZILLA will be very useful for our
project.

}
