\section{Related Work}
\label{sec-related}

\paragraph{Day of Week of Commits}
The most closely related work studied the day of week of commits and
showed that the commits on Fridays are buggier~\cite{sliwerski-msr-2005}. 
This paper differs from it mainly in the following three aspects.
Firstly, we investigated how the commits' time of day correlates with the bugginess of commits, 
which has not been studied before to the best of our knowledge.
Secondly, we studied the correlation between commit bugginess and developers' activity levels as well as 
developer experience, which the previous work did not study.
Lastly, we used different data collection techniques. Specifically, we did not reply on the link 
between a commit and a bug report to extract bug-fixing commits, which enabled us to study 
software for which such links are not maintained or not well maintained by the developers. For example, we 
found that only 2.3\% of the bug-fixing commits are linked to a bug report, 
through manually examining a random sample of our bug-fixing commits in the Linux kernel.
While using links between bug reports and bug commits may increase the precision of extracting 
bug-fixing commits, our results demonstrate that 
high precision can be obtained without using such links: 
the precision of our bug-fixing commit extraction techniques are 
\linuxP for the Linux kernel and \postP for PostgreSQL.
%(1/43= 2.3\%)

\paragraph{Empirical Studies on Commits}
Previous study~\cite{largeCommits} classifies commits into different categories, one of which 
is non-functional commits (e.g., modification of comments, documentations, etc.).
Our study is different because we focused on
comment-fixing commits. For example, a refactoring commit 
%that modifies the source code but not comments 
would be considered as a non-functional commit by previous study, but not considered as a comment-fixing commits in this paper, 
thus being excluded from our study. 
iComment~\cite{iComment} only showed that FreeBSD contains many comment-fixing commits, and it is 
not a comprehensive study on comment-fixing commits.  
Our results show that XXX of commits in the Linux kernel and PostgreSQL only fix comments, and do not modify the code, showing
that developers spent time maintaining the correctness of comments. 

% cite ahmed hassan's work

\comment{ 
\paragraph{When do changed induce fixes?}

One of the relevant academic works is this paper, which analyzes CVS
achieves for fix-inducing changes. In other words, they examine code
changes that lead to problems. They discuss a methodology to
automatically locate fix-inducing changes by linking a version archive
to a bug database such as Bugzilla~\cite{sliwerski-msr-2005}. The authors
examined the history for Mozilla and Eclipse for their data
collection. The results they collected yielded that fix-inducing
changes show distinct patterns with respect to their size and the day
of the week they occurred. They discuss the general idea behind the
process of finding fix-inducing changes as: 1. Start with a bug report
in the bug database, indicating a fixed problem, extract the
associated change from the version archive to get the location of the
fix, and finally determine the earlier change at this location that
was applied before the bug was reported. The authors describe
syntactic and semantic analysis for the first step in the their
process of identifying potential fixes. Finally, through the use of
diff and annotate commands, as well as cycling through different
versions of the code, the authors are able to locate fix-inducing
changes. The study concluded that most bugs are introduced on Fridays
and Sundays. This work was important because it provided us with a
guideline for a methodology for extracting fix-inducing changes.

\paragraph{Automatic identification of bug-introducing changes}
The authors try to automate the process of finding bug-introducing
changes, which would remove the manual work associated with going
through bug reports or commit logs to collect this type of
information. They use the SZZ algorithm, which traces from the
location of the fix where the bug was introduced, and as such extract
the time \cite{2006-automatic}.The weakness with this algorithm is
that sometimes it makes false detections because not all modifications
are fixes, and a moderate improvement from using just SZZ is the use
of annotation graphs. Another improvement was ignoring format changes,
which reduce a large number of false positives. This paper has
inspired the approach we used for automating the process of
identifying bug-introducing changes, which was key for collecting a
large pool of data.

\paragraph{How long did it take to fix bugs?}
In this paper the author try to measure software quality as a function
of the number of bugs. The authors examine the bug fix time of files
in two open source software: ArgoUML and PostgreSQL, and tackle this
issue by identifying when bugs are introduced and when the bugs are
fixed \cite{2006-long}. The argument is files with the greatest
bug-fix times, whose bug counts are greater than average, may need
more attention to determine why bug fixes take such a long time –
potentially indicating the need for code refactoring to achieve faster
bug fixes in the future. The authors first extracted change histories
of the two projects, ArgoUML and PostgreSQL, using the Kenyon
infrastructure. To identify a bug-fix, they searched for keywords such
as fixed or bugs and they also searched for references to bug
reports. They then applied the identified bug-introducing changes by
applying the fix-inducing change identification algorithms described
in the paper “When do changes induce fixes?” which I mentioned
earlier.  This work is of course relevant because it gives us a
methodology for extracting bug-fix times.

\paragraph{If your bug database could talk}
This is another relevant work, where the authors perform experiments
that demonstrate how to relate developer, code, and process to defects
in the code. This work tries to understand why some programs are more
failure-prone than others.  To answer this question, we have to know
which programs are more failure-prone than others – to search for
properties of the programs or its development process that commonly
correlate with defect density. The authors try to answer questions
like “can one predict failure-proneness from metrics like code
complexity?”, “what does a high number of bugs found after release?”,
and “do some developers write more failure-prone code than
others?”. After examining Eclipse database, some of the conclusions
that the authors made were: new or combination of existing metrics
need to be explored to study the relationship between complexity of
code to the presence of bugs in a given class, it is difficult to
predict post-release failures solely from process measurements, and
there is a high variance in failure density in files owned by
different developers \cite{2006-if}. Their methodology for extracting
information from bug reports from BUGZILLA will be very useful for our
project.

\paragraph{On the Nature of Commits}
This paper studies the nature of commits in two dimensions: define the
size of commits in terms of number of files, and classify commits
based on the content of their comments \cite{hattori2008nature}. The
authors investigated the distribution of commits according to the
number of files, and their results show that the majority of commits
contain a large number of files. The authors also developed a
classification system for commits according to development and
maintenance activities based on the content of their commits, a system
that is more suitable for open source projects. Some of the major
findings made by the authors include: the majority of the commits are
not related to the development activities, corrective actions generate
more tiny commits, and development activities are spread among all
sizes of commits.
}
